---
title: "AI実装検定A級のメモ"
date: 2021-04-26T16:28:05+09:00
lastmod: 2021-04-26T16:28:05+09:00
draft: false
thumbnail: "images/noimage.jpg"
categories: [ "others" ]
tags: [ "メモ","追記予定" ]
---


## AI概論


### 特徴量とは

特徴量とは、物体が物体であると認識できるもののことを言う。

例えば、犬の画像をAIが犬と認識するには、「耳が尖っている」「四足歩行である」「体毛に覆われている」などを特徴量と言い、それを認識している必要がある。

### 教師なし学習と教師あり学習


- 教師あり学習: 入力データと正解ラベルのセットを使って学習する(※犬・猫の画像をそれぞれ、犬、猫とつけたラベルのことを正解ラベルという)
- 教師なし学習: 正解ラベルがないデータの集まりで学習をする(似た画像同士でグループ分け(クラスタリング)をする必要がある)


#### 代表的な教師あり学習

- 決定木 (ラベルをもとに分類)
- サポートベクターマシン(マージン最大化による分類)
- 線形回帰(回帰モデル)


#### 代表的な教師なし学習

- k-means(クラスタリング、似たデータを自動でグループ化)


### 標本化と量子化

標本化: 一定間隔でサンプルリングし、ピクセル単位に分解すること
量子化: 各ピクセルの色の強さを、有限の数値に変換する処理

まず、標本化で、一定間隔で区切って分解をする。

続いて、量子化で、色の強さを0~255に丸める。

この標本化と量子化が画像認識において必須の前処理という作業。

標本化 → 量子化 → 入力データ作成 という工程を経て、モデルにデータを与え判定をすることができる。




### 誤差逆伝播法(バックプロパゲーション)

ニューラルネットワークにおいて、出力と正解の誤差を、ネットワークを逆にたどって重みを修正する方法。

このとき、どのように重みを変化させるかを微分(勾配)を用いて計算する。

### バイアス項

バイアスとは、出力をずらすための定数項

例えば、y= Wx+b という重みつき入力Wxに対して、bというバイアスを与えて出力値をずらす。


## 数学


### 試験に出る代表的な公式


#### 微分の基本公式

<div class="img-center"><img src="/images/Screenshot from 2025-04-26 16-56-38.png" alt=""></div>

#### 合成関数の微分


<div class="img-center"><img src="/images/Screenshot from 2025-04-26 16-56-43.png" alt=""></div>

#### 極限の基本公式

<div class="img-center"><img src="/images/Screenshot from 2025-04-26 16-56-47.png" alt=""></div>


#### 指数・対数の公式

<div class="img-center"><img src="/images/Screenshot from 2025-04-26 16-57-04.png" alt=""></div>

#### 勾配降下法

<div class="img-center"><img src="/images/Screenshot from 2025-04-26 16-59-04.png" alt=""></div>

勾配降下法とは損失を小さくなる方向に少しずつパラメータを調整していく方法。

損失関数とはAIの予測と正解とのズレ(誤差)を数値で表現したもの。この損失関数を最小にすることがAI開発の目標である(高い正解率)

- 損失: 誤判定
- 損失関数: 損失(誤判定)を数値化したもの
- 勾配降下法: 損失(誤判定)を最小にするために使うもの


#### ベクトル・ノルムの基本

<div class="img-center"><img src="/images/Screenshot from 2025-04-26 16-59-07.png" alt=""></div>

##### ベクトル

向きと大きさを持つ物。例えば(3, 4)というベクトルは、原点から横に3，縦に4進むという意味がある。この2つの要素があるベクトルを2次元ベクトルという。

1x4で4つの要素を持つベクトルを、4次元ベクトルという。

このベクトルは行列の一種である。(横、縦に長い行列。)

##### ノルム

ノルムはベクトルの長さを表現したもの。

L2ノルム(ユークリッド距離)はピタゴラスの定理(a^2 + b^2 = c^2)で求めることができる。

例えば (3, 4)であれば、

このように求めることができる。

<div class="img-center"><img src="/images/Screenshot from 2025-04-26 17-08-14.png" alt=""></div>


L1ノルム(タクシー距離)は、縦横にしか移動できない場合の移動距離の合計

(3,4)であれば、縦方向に4、横方向に3なので、足して7となる。

単にノルムとだけ書かれていた場合、一般的にはL2ノルムを意味している。


<div class="img-center"><img src="" alt=""></div>
<div class="img-center"><img src="" alt=""></div>
<div class="img-center"><img src="" alt=""></div>
<div class="img-center"><img src="" alt=""></div>
<div class="img-center"><img src="" alt=""></div>



